# my-bigdata-stack\docker-compose.yml
version: "3.8"

networks:
  bigdata-net:
    name: bigdata-net
    driver: bridge

volumes:
  hadoop_namenode_data:
  hadoop_datanode_data:
  hadoop_historyserver_data:
  hbase_data:
  postgres_metastore_data:
  zookeeper_data:
  zookeeper_log:

services:
  # ----------------- Builder Services (only for building images) -----------------
  base-builder:
    image: my-bigdata-base:latest
    build:
      context: .
      dockerfile: services/base/Dockerfile
    profiles: ["build"]

  hadoop-builder:
    image: bigdata-hadoop-base:latest
    build:
      context: .
      dockerfile: services/hadoop-base/Dockerfile
      args:
        - HADOOP_VERSION=${HADOOP_VERSION}
    depends_on:
      - base-builder
    profiles: ["build"]

  hbase-builder:
    image: bigdata-hbase:latest
    build:
      context: .
      dockerfile: services/hbase/Dockerfile
      args:
        - HBASE_VERSION=${HBASE_VERSION}
    depends_on:
      - hadoop-builder
    profiles: ["build"]

  hive-builder:
    image: bigdata-hive:latest
    build:
      context: .
      dockerfile: services/hive/Dockerfile
      args:
        - HIVE_VERSION=${HIVE_VERSION}
        - PG_JDBC_VERSION=${PG_JDBC_VERSION}
    depends_on:
      - hadoop-builder
    profiles: ["build"]

  spark-builder:
    image: bigdata-spark:latest
    build:
      context: .
      dockerfile: services/spark/Dockerfile
      args:
        - SPARK_VERSION=${SPARK_VERSION}
    depends_on:
      - hadoop-builder
    profiles: ["build"]

  # ----------------- Runtime Services (these will actually run) -----------------
  zookeeper:
    image: zookeeper:${ZOOKEEPER_VERSION}
    container_name: zookeeper
    hostname: zookeeper
    networks:
      - bigdata-net
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/data
      - zookeeper_log:/datalog
    environment:
      - ZOO_SERVER_ID=1

  postgres-metastore:
    image: postgres:14
    container_name: postgres-metastore
    hostname: postgres-metastore
    networks:
      - bigdata-net
    ports:
      - "5432:5432"
    volumes:
      - postgres_metastore_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}

  namenode:
    image: bigdata-hadoop-base:latest
    container_name: namenode
    hostname: namenode
    command: ["hdfs", "namenode"]
    networks:
      - bigdata-net
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - ./configs/hadoop:/etc/hadoop
      - hadoop_namenode_data:/opt/hadoop/data/namenode
    environment:
      - HADOOP_USER_NAME=root
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://namenode:9870 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  datanode:
    image: bigdata-hadoop-base:latest
    depends_on:
      namenode:
        condition: service_healthy
    container_name: datanode
    hostname: datanode
    command: ["hdfs", "datanode"]
    networks:
      - bigdata-net
    volumes:
      - ./configs/hadoop:/etc/hadoop
      - hadoop_datanode_data:/opt/hadoop/data/datanode

  resourcemanager:
    image: bigdata-hadoop-base:latest
    depends_on:
      datanode:
        condition: service_started
    container_name: resourcemanager
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    networks:
      - bigdata-net
    ports:
      - "8088:8088"
    volumes:
      - ./configs/hadoop:/etc/hadoop
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://resourcemanager:8088 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  nodemanager:
    image: bigdata-hadoop-base:latest
    depends_on:
      resourcemanager:
        condition: service_healthy
    container_name: nodemanager
    hostname: nodemanager
    command: ["yarn", "nodemanager"]
    networks:
      - bigdata-net
    volumes:
      - ./configs/hadoop:/etc/hadoop
# 依赖init-hdfs.sh 脚本执行
  historyserver:
    image: bigdata-hadoop-base:latest
    depends_on:
      resourcemanager:
        condition: service_healthy
    container_name: historyserver
    hostname: historyserver
    command: ["mapred", "historyserver"]
    networks:
      - bigdata-net
    ports:
      - "19888:19888"
    volumes:
      - ./configs/hadoop:/etc/hadoop

  hbase-master:
    image: bigdata-hbase:latest
    depends_on:
      namenode:
        condition: service_healthy
      zookeeper:
        condition: service_started
    container_name: hbase-master
    hostname: hbase-master
    # 修改这里的command
    command: ["/opt/hbase/bin/hbase", "master", "start"]
    networks:
      - bigdata-net
    ports:
      - "16010:16010"
    volumes:
      - ./configs/hbase:/opt/hbase/conf
      - ./configs/hadoop:/etc/hadoop

  hbase-regionserver:
    image: bigdata-hbase:latest
    depends_on:
      hbase-master:
        condition: service_started
    container_name: hbase-regionserver
    hostname: hbase-regionserver
    # 修改这里的command
    command: ["/opt/hbase/bin/hbase", "regionserver", "start"]
    networks:
      - bigdata-net
    volumes:
      - ./configs/hbase:/opt/hbase/conf
      - ./configs/hadoop:/etc/hadoop

  hive-metastore:
    image: bigdata-hive:latest
    depends_on:
      postgres-metastore:
        condition: service_started
      namenode:
        condition: service_healthy
    container_name: hive-metastore
    hostname: hive-metastore
    command: ["metastore"]
    networks:
      - bigdata-net
    ports:
      - "9083:9083"
    volumes:
      - ./configs/hive:/opt/hive/conf
      - ./configs/hadoop:/etc/hadoop
  hiveserver2:
    image: bigdata-hive:latest
    depends_on:
      hive-metastore:
        condition: service_started
      resourcemanager:
        condition: service_healthy
    container_name: hiveserver2
    hostname: hiveserver2
    command: ["hiveserver2"]
    networks:
      - bigdata-net
    ports:
      - "10000:10000"
      - "10002:10002"
    volumes:
      - ./configs/hive:/opt/hive/conf
      - ./configs/hadoop:/etc/hadoop
  spark-client:
    image: bigdata-spark:latest
    depends_on:
      - hiveserver2
      - hbase-master
    container_name: spark-client
    hostname: spark-client
    command: tail -f /dev/null
    tty: true
    stdin_open: true

    networks:
      - bigdata-net
    volumes:
      - ./configs/spark:/opt/spark/conf
      - ./configs/hadoop:/etc/hadoop
      - ./configs/hive:/opt/hive/conf