# my-bigdata-stack/docker-compose.yml
networks:
  bigdata-net:
    name: bigdata-net
    driver: bridge

volumes:
  hadoop_namenode_data:
  hadoop_datanode_data:
  hadoop_historyserver_data:
  hbase_data:
  postgres_metastore_data:
  zookeeper_data:
  zookeeper_log:
  zookeeper_logs:
  kafka_data:
  flink_data:
  airflow_dags:
  airflow_logs:
  airflow_plugins:

services:
  # ----------------- Builder Services (仅用于构建镜像) -----------------
  base-builder:
    image: my-bigdata-base:latest
    build:
      context: .
      dockerfile: services/base/Dockerfile
    profiles: ["build"]

  hadoop-builder:
    image: bigdata-hadoop-base:latest
    build:
      context: .
      dockerfile: services/hadoop-base/Dockerfile
      args:
        - HADOOP_VERSION=${HADOOP_VERSION}
    depends_on:
      - base-builder
    profiles: ["build"]

  hbase-builder:
    image: bigdata-hbase:latest
    build:
      context: .
      dockerfile: services/hbase/Dockerfile
      args:
        - HBASE_VERSION=${HBASE_VERSION}
    depends_on:
      - hadoop-builder
    profiles: ["build"]

  hive-builder:
    image: bigdata-hive:latest
    build:
      context: .
      dockerfile: services/hive/Dockerfile
      args:
        - HIVE_VERSION=${HIVE_VERSION}
        - PG_JDBC_VERSION=${PG_JDBC_VERSION}
    depends_on:
      - hadoop-builder
    profiles: ["build"]

  spark-builder:
    image: bigdata-spark:latest
    build:
      context: .
      dockerfile: services/spark/Dockerfile
      args:
        - SPARK_VERSION=${SPARK_VERSION}
    depends_on:
      - hadoop-builder
    profiles: ["build"]

  # ----------------- Runtime Services (实际运行的服务) -----------------
  zookeeper:
    image: zookeeper:${ZOOKEEPER_VERSION}
    container_name: zookeeper
    hostname: zookeeper
    networks:
      - bigdata-net
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/data
      - zookeeper_log:/datalog
      - zookeeper_logs:/logs
    environment:
      - ZOO_SERVER_ID=1
      - ZOO_MAX_CLIENT_CNXNS=0
      - ZOO_JUTE_MAXBUFFER=41943040

  postgres-metastore:
    image: postgres:17.5
    container_name: postgres-metastore
    hostname: postgres-metastore
    networks:
      - bigdata-net
    ports:
      - "5432:5432"
    volumes:
      - postgres_metastore_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}

  namenode:
    image: bigdata-hadoop-base:latest
    container_name: namenode
    hostname: namenode
    command: ["hdfs", "namenode"]
    networks:
      - bigdata-net
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - ./configs/hadoop:/etc/hadoop
      - hadoop_namenode_data:/opt/hadoop/data/namenode
    environment:
      - HADOOP_USER_NAME=root
      - HADOOP_OPTS=-Djava.net.preferIPv4Stack=true
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://namenode:9870 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  datanode:
    image: bigdata-hadoop-base:latest
    depends_on:
      namenode:
        condition: service_healthy
    container_name: datanode
    hostname: datanode
    command: ["hdfs", "datanode"]
    networks:
      - bigdata-net
    volumes:
      - ./configs/hadoop:/etc/hadoop
      - hadoop_datanode_data:/opt/hadoop/data/datanode
    environment:
      - HADOOP_OPTS=-Djava.net.preferIPv4Stack=true

  resourcemanager:
    image: bigdata-hadoop-base:latest
    depends_on:
      datanode:
        condition: service_started
    container_name: resourcemanager
    hostname: resourcemanager
    command: [ "yarn", "resourcemanager" ]
    networks:
      - bigdata-net
    ports:
      - "8088:8088"
    volumes:
      - ./configs/hadoop:/etc/hadoop
    environment:
      - HADOOP_OPTS=-Djava.net.preferIPv4Stack=true
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://resourcemanager:8088 || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5

  nodemanager:
    image: bigdata-hadoop-base:latest
    depends_on:
      resourcemanager:
        condition: service_healthy
    container_name: nodemanager
    hostname: nodemanager
    command: ["yarn", "nodemanager"]
    networks:
      - bigdata-net
    volumes:
      - ./configs/hadoop:/etc/hadoop
    environment:
      - HADOOP_OPTS=-Djava.net.preferIPv4Stack=true

  historyserver:
    image: bigdata-hadoop-base:latest
    depends_on:
      resourcemanager:
        condition: service_healthy
    container_name: historyserver
    hostname: historyserver
    command: ["mapred", "historyserver"]
    networks:
      - bigdata-net
    ports:
      - "19888:19888"
    volumes:
      - ./configs/hadoop:/etc/hadoop
    environment:
      - HADOOP_OPTS=-Djava.net.preferIPv4Stack=true

  hbase-master:
    image: bigdata-hbase:latest
    depends_on:
      namenode:
        condition: service_healthy
      zookeeper:
        condition: service_started
    container_name: hbase-master
    hostname: hbase-master
    command: ["master", "start"]
    networks:
      - bigdata-net
    ports:
      - "16010:16010"
    volumes:
      - ./configs/hbase/hbase-site.xml:/opt/hbase/conf/hbase-site.xml
      - ./configs/hbase/hbase-env.sh:/opt/hbase/conf/hbase-env.sh
      - ./configs/hadoop:/etc/hadoop
    environment:
      - HBASE_OPTS=-Djava.net.preferIPv4Stack=true

  hbase-regionserver:
    image: bigdata-hbase:latest
    depends_on:
      hbase-master:
        condition: service_started
    container_name: hbase-regionserver
    hostname: hbase-regionserver
    command: ["regionserver", "start"]
    networks:
      - bigdata-net
    volumes:
      - ./configs/hbase/hbase-site.xml:/opt/hbase/conf/hbase-site.xml
      - ./configs/hbase/hbase-env.sh:/opt/hbase/conf/hbase-env.sh
      - ./configs/hadoop:/etc/hadoop
    environment:
      - HBASE_OPTS=-Djava.net.preferIPv4Stack=true

  hive-metastore:
    image: bigdata-hive:latest
    depends_on:
      postgres-metastore:
        condition: service_started
      namenode:
        condition: service_healthy
    container_name: hive-metastore
    hostname: hive-metastore
    command: ["metastore"]
    networks:
      - bigdata-net
    ports:
      - "9083:9083"
    volumes:
      - ./configs/hive/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./configs/hive/log4j2.properties:/opt/hive/conf/log4j2.properties
      - ./configs/hive/hive-exec-log4j2.properties:/opt/hive/conf/hive-exec-log4j2.properties
      - ./configs/hive/beeline-log4j2.properties:/opt/hive/conf/beeline-log4j2.properties
      - ./configs/hadoop:/etc/hadoop
    environment:
      - HIVE_LOG_CONF_DIR=/opt/hive/conf
      - HADOOP_OPTS=-Djava.net.preferIPv4Stack=true

  hiveserver2:
    image: bigdata-hive:latest
    depends_on:
      hive-metastore:
        condition: service_started
      resourcemanager:
        condition: service_healthy
    container_name: hiveserver2
    hostname: hiveserver2
    command: ["hiveserver2"]
    networks:
      - bigdata-net
    ports:
      - "10000:10000"
      - "10002:10002"
    volumes:
      - ./configs/hive/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./configs/hive/log4j2.properties:/opt/hive/conf/log4j2.properties
      - ./configs/hive/hive-exec-log4j2.properties:/opt/hive/conf/hive-exec-log4j2.properties
      - ./configs/hive/beeline-log4j2.properties:/opt/hive/conf/beeline-log4j2.properties
      - ./configs/hadoop:/etc/hadoop
    environment:
      - HIVE_LOG_CONF_DIR=/opt/hive/conf
      - HADOOP_OPTS=-Djava.net.preferIPv4Stack=true

  spark-client:
    image: bigdata-spark:latest
    entrypoint: ["/opt/entrypoint.sh"]
    command: tail -f /dev/null
    depends_on:
      - hiveserver2
      - hbase-master
    container_name: spark-client
    hostname: spark-client
    tty: true
    stdin_open: true
    networks:
      - bigdata-net
    volumes:
      - ./configs/spark:/opt/spark/conf
      - ./configs/hadoop:/etc/hadoop
      - ./configs/hive:/opt/hive/conf
    environment:
      - SPARK_DIST_CLASSPATH=/opt/hadoop/etc/hadoop
      - SPARK_OPTS=-Djava.net.preferIPv4Stack=true

  # ----------------- Kafka (实时数据总线) -----------------
  kafka:
    image: bitnami/kafka:3.5.1
    container_name: kafka
    hostname: kafka
    networks:
      - bigdata-net
    ports:
      - "9092:9092"
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_SOCKET_REQUEST_MAX_BYTES=524288000
    depends_on:
      - zookeeper
    volumes:
      - kafka_data:/bitnami/kafka

  # ----------------- Flink (实时计算引擎) -----------------
  flink-jobmanager:
    image: flink:1.17-scala_2.12
    container_name: flink-jobmanager
    hostname: flink-jobmanager
    networks:
      - bigdata-net
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - JVM_OPTS=-Djava.net.preferIPv4Stack=true
    volumes:
      - ./configs/flink/flink-jobmanager.yaml:/opt/flink/conf/flink-conf.yaml
      - ./flink-jobs:/opt/flink-jobs
    depends_on:
      namenode:
        condition: service_healthy
      resourcemanager:
        condition: service_healthy
      kafka:
        condition: service_started

  flink-taskmanager:
    image: flink:1.17-scala_2.12
    container_name: flink-taskmanager
    hostname: flink-taskmanager
    networks:
      - bigdata-net
    command: taskmanager
    environment:
      - JVM_OPTS=-Djava.net.preferIPv4Stack=true
    volumes:
      - ./configs/flink/flink-taskmanager.yaml:/opt/flink/conf/flink-conf.yaml
      - ./flink-jobs:/opt/flink-jobs
    depends_on:
      - flink-jobmanager

  # ----------------- Airflow (工作流编排器) -----------------
  airflow-scheduler:
    image: apache/airflow:2.8.1
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    networks:
      - bigdata-net
    command: scheduler
    depends_on:
      postgres-metastore:
        condition: service_started
      kafka:
        condition: service_started
      namenode:
        condition: service_healthy
      resourcemanager:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres-metastore:5432/${POSTGRES_DB}
      - AIRFLOW__CORE__FERNET_KEY=pKxRNfT4l8YYlkYMtDGtrdY4TkcBRLVOjhnIkGB0GrQ=
      - AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.default
    volumes:
      - ./dags:/opt/airflow/dags
      - ./airflow-logs:/opt/airflow/logs
      - ./airflow-plugins:/opt/airflow/plugins

  airflow-webserver:
    image: apache/airflow:2.8.1
    container_name: airflow-webserver
    hostname: airflow-webserver
    networks:
      - bigdata-net
    ports:
      - "8080:8080"
    command: webserver
    depends_on:
      - postgres-metastore
      - kafka
      - namenode
      - resourcemanager
    environment:
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres-metastore:5432/${POSTGRES_DB}
      - AIRFLOW__CORE__FERNET_KEY=pKxRNfT4l8YYlkYMtDGtrdY4TkcBRLVOjhnIkGB0GrQ=
      - AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.default
    volumes:
      - ./dags:/opt/airflow/dags
      - ./airflow-logs:/opt/airflow/logs
      - ./airflow-plugins:/opt/airflow/plugins